<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Thong T. Nguyen


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒŽ</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-217568923-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-217568923-1');
</script>




  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  
</head>

<script>function toggleblock(blockId) {
    var block = document.getElementById(blockId);
    if (block.style.display == 'none') {
      block.style.display = 'block';
    } else {
      block.style.display = 'none';
    }
  }</script>

<body
  class="fixed-top-nav ">

  <!-- Header -->

  <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              About
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


  <!-- Content -->

  <div class="container mt-5">
    <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Thong</span> T. Nguyen
    </h1>
     <p class="desc">PhD student at National University of Singapore</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/thong_nguyen.jpg">
      
      
    </div>
    

    <div class="clearfix">
      <p>I am a PhD student advised by Professor <a href="https://www.comp.nus.edu.sg/~ngsk/">See-Kiong Ng</a> and Professor <a href="https://tuanluu.github.io/">Anh-Tuan Luu</a> at National University of Singapore, where I was fortunate enough to be funded by a <a href="https://research.google/outreach/phd-fellowship/recipients/">Google PhD Fellowship</a> and AI Singapore PhD Fellowship. My research interests lie in the intersection of Natural Language Processing and Computer Vision with the focus on Large Multimodal Model and Video Understanding.</p>

<p><span style="font-weight: 700!important">Contact</span>: <span style="font-family: 'Lucida Console', monospace">e0998147 [at] u.nus.edu</span></p>

<p><span style="font-weight: 700!important">Photo credit</span>: <span style="font-family: 'Lucida Console', monospace"><a href="https://www.instagram.com/abbymyth/">abbymyth<i class="material-icons" style="font-size:15px;color:#fec44f">wb_sunny</i></a></span></p>

    </div>

    
      <div class="news">
  <h2>News</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Sep 2, 2025</th>
          <td>
            
              Succesfully defended my PhD thesis: <strong>Video Understanding - Through a Temporal Lens</strong>

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Apr 9, 2025</th>
          <td>
            
              Invited to serve as Area Chair for NeurIPS 2025.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 10, 2024</th>
          <td>
            
              Two papers accepted to AAAI 2025.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 25, 2024</th>
          <td>
            
              Invited to serve as Area Chair for NAACL 2025.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 26, 2024</th>
          <td>
            
              One paper accepted to NeurIPS 2024.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Sep 20, 2024</th>
          <td>
            
              One paper accepted to EMNLP 2024.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 2, 2024</th>
          <td>
            
              One paper accepted to ECCV 2024.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jun 21, 2024</th>
          <td>
            
              Invited to serve as Area Chair for EMNLP 2024.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">May 16, 2024</th>
          <td>
            
              Two papers accepted to ACL 2024 (Findings).

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Mar 14, 2024</th>
          <td>
            
              One paper accepted to NAACL 2024.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 20, 2023</th>
          <td>
            
              One paper accepted to Artificial Intelligence Review.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 10, 2023</th>
          <td>
            
              Two papers accepted to AAAI 2024.

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>Selected publications</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">Under Review</abbr>
    
  
  </div>

  <div id="nguyen2025temporal" class="col-sm-8">
    
      <div class="title">Temporal-Oriented Recipe for Transferring Large Vision-Language Model to Video Understanding</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <span style="font-weight: 600;">Thong Nguyen</span>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhiyuan Hu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xu Lin,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Cong-Duy Nguyen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  See-Kiong Ng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Luu Anh Tuan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Under Review</em>
      
      
        <i>2025</i>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2505.12605" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
      <a href="https://nguyentthong.github.io/temporal_recipe/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent years have witnessed outstanding advances of large vision-language models (LVLMs). In order to tackle video understanding, most of them depend upon their implicit temporal understanding capacity. As such, they have not deciphered important components that contribute to temporal understanding ability, which might limit the potential of these LVLMs for video understanding. In this work, we conduct a thorough empirical study to demystify crucial components that influence the temporal understanding of LVLMs. Our empirical study reveals that significant impacts are centered around the intermediate interface between the visual encoder and the large language model. Building on these insights, we propose a temporal-oriented recipe that encompasses temporal-oriented training schemes and an upscaled interface. Our final model developed using our recipe significantly enhances previous LVLMs on standard video understanding tasks.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
    
  
  </div>

  <div id="nguyen2024motion" class="col-sm-8">
    
      <div class="title">Motion-aware Contrastive Learning for Temporal Panoptic Scene Graph Generation</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <span style="font-weight: 600;">Thong Nguyen</span>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiaobao Wu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yi Bin,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Cong-Duy Nguyen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  See-Kiong Ng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Anh Tuan Luu
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of AAAI</em>
      
      
        <i>2025</i>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2412.07160" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>To equip artificial intelligence with a comprehensive understanding towards a temporal world, video and 4D panoptic scene graph generation abstracts visual data into nodes to represent entities and edges to capture temporal relations. Existing methods encode entity masks tracked across temporal dimensions (mask tubes), then predict their relations with temporal pooling operation, which does not fully utilize the motion indicative of the entitiesâ€™ relation. To overcome this limitation, we introduce a contrastive representation learning framework that focuses on motion pattern for temporal scene graph generation. Firstly, our framework encourages the model to learn close representations for mask tubes of similar subject-relation-object triplets. Secondly, we seek to push apart mask tubes from their temporally shuffled versions. Moreover, we also learn distant representations for mask tubes belonging to the same video but different triplets. Extensive experiments show that our motion-aware contrastive framework significantly improves state-of-the-art methods on both video and 4D datasets.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
    
  
  </div>

  <div id="nguyen2024multi" class="col-sm-8">
    
      <div class="title">Multi-Scale Contrastive Learning for Video Temporal Grounding</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <span style="font-weight: 600;">Thong Nguyen</span>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yi Bin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiaobao Wu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhiyuan Hu,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Cong-Duy Nguyen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  See-Kiong Ng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Anh Tuan Luu
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of AAAI</em>
      
      
        <i>2025</i>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2412.07157" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Temporal grounding, which localizes video moments related to a natural language query, is a core problem of vision-language learning and video understanding. To encode video moments of varying lengths, recent methods employ a multi-level structure known as a feature pyramid. In this structure, lower levels concentrate on short-range video moments, while higher levels address long-range moments. Because higher levels experience downsampling to accommodate increasing moment length, their capacity to capture information is reduced and consequently leads to degraded information in moment representations. To resolve this problem, we propose a contrastive learning framework to capture salient semantics among video moments. Our key methodology is to leverage samples from the feature space emanating from multiple stages of the video encoder itself requiring neither data augmentation nor online memory banks to obtain positive and negative samples. To enable such an extension, we introduce a sampling process to draw multiple video moments corresponding to a common query. Subsequently, by utilizing these momentsâ€™ representations across video encoder layers, we instantiate a novel form of multi-scale and cross-scale contrastive learning that links local short-range video moments with global long-range video moments. Extensive experiments demonstrate the effectiveness of our framework for not only long-form but also short-form video grounding.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="nguyen2024encoding" class="col-sm-8">
    
      <div class="title">Encoding and Controlling Global Semantics for Long-form Video Question Answering</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <span style="font-weight: 600;">Thong Nguyen</span>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhiyuan Hu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiaobao Wu,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Cong-Duy Nguyen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  See-Kiong Ng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Luu Anh Tuan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of EMNLP</em>
      
      
        <i>2024</i>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2405.19723" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/zhiyuanhubj/long_form_videoqa" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      <a href="https://nguyentthong.github.io/Long_form_VideoQA/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Seeking answers effectively for long videos is essential to build video question answering (videoQA) systems. Previous methods adaptively select frames and regions from long videos to save computations. However, this fails to reason over the whole sequence of video, leading to sub-optimal performance. To address this problem, we introduce a state space layer (SSL) into multi-modal Transformer to efficiently integrate global semantics of the video, which mitigates the video information loss caused by frame and region selection modules. Our SSL includes a gating unit to enable controllability over the flow of global semantics into visual representations. To further enhance the controllability, we introduce a cross-modal compositional congruence (C^3) objective to encourage global semantics aligned with the question. To rigorously evaluate long-form videoQA capacity, we construct two new benchmarks Ego-QA and MAD-QA featuring videos of considerably long length, i.e. 17.5 minutes and 1.9 hours, respectively. Extensive experiments demonstrate the superiority of our framework on these new as well as existing datasets.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ECCV</abbr>
    
  
  </div>

  <div id="nguyen2024meta" class="col-sm-8">
    
      <div class="title">MAMA: A Meta-optimized Angular Margin Contrastive Framework for Video-Language Representation Learning</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <span style="font-weight: 600;">Thong Nguyen</span>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yi Bin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiaobao Wu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xinshuai Dong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhiyuan Hu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Khoi Le,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Cong-Duy Nguyen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  See-Kiong Ng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Luu Anh Tuan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of ECCV</em>
      
      
        <i>2024</i>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2407.03788" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/nguyentthong/MAMA" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      <a href="https://nguyentthong.github.io/MAMA/" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    
      <a href="https://huggingface.co/spaces/thongnguyen5999/mama/" class="btn btn-sm z-depth-0" role="button" target="_blank">Demo</a>
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Data quality stands at the forefront of deciding the effectiveness of video-language representation learning. However, video-text pairs in previous data typically do not align perfectly with each other, which might lead to video-language representations that do not accurately reflect cross-modal semantics. Moreover, previous data also possess an uneven distribution of concepts, thereby hampering the downstream performance across unpopular subjects. To address these problems, we propose a contrastive objective with a subtractive angular margin to regularize cross-modal representations in their effort to reach perfect similarity. Furthermore, to adapt to the non-uniform concept distribution, we propose a multi-layer perceptron (MLP)-parameterized weighting function that maps loss values to sample weights which enable dynamic adjustment of the modelâ€™s focus throughout the training. With the training guided by a small amount of unbiased meta-data and augmented by video-text data generated by large vision-language model, we improve video-language representations and achieve superior performances on commonly used video question answering and text-video retrieval datasets.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="nguyen2024video" class="col-sm-8">
    
      <div class="title">Video-Language Understanding: A Survey from Model Architecture, Model Training, and Data Perspectives</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <span style="font-weight: 600;">Thong Nguyen</span>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yi Bin,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Junbin Xiao,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Leigang Qu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Yicong Li,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Jay Zhangjie Wu,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Cong-Duy Nguyen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  See-Kiong Ng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Luu Anh Tuan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of ACL (Findings)</em>
      
      
        <i>2024</i>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2406.05615" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/nguyentthong/video-language-understanding" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Humans use multiple senses to comprehend the environment. Vision and language are two of the most vital senses since they allow us to easily communicate our thoughts and perceive the world around us. There has been a lot of interest in creating video-language understanding systems with human-like senses since a video-language pair can mimic both our linguistic medium and visual environment with temporal dynamics. In this survey, we review the key tasks of these systems and highlight the associated challenges. Based on the challenges, we summarize their methods from model architecture, model training, and data perspectives. We also conduct performance comparison among the methods, and discuss promising directions for future research.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
    
  
  </div>

  <div id="nguyen2023read" class="col-sm-8">
    
      <div class="title">READ: Recurrent Adapter with Partial Video-Language Alignment for Parameter-Efficient Transfer Learning in Low-Resource Video-Language Modeling</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <span style="font-weight: 600;">Thong Nguyen</span>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiaobao Wu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xinshuai Dong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Khoi Le,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhiyuan Hu,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Cong-Duy Nguyen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  See-Kiong Ng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Luu Anh Tuan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of AAAI</em>
      
      
        <i>2024</i>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2312.06950" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/nguyentthong/READ" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
      <a href="https://nguyentthong.github.io/READ" class="btn btn-sm z-depth-0" role="button" target="_blank">Website</a>
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Fully fine-tuning pretrained large-scale transformer models has become a popular paradigm for video-language modeling tasks, such as temporal language grounding and video-language summarization. With a growing number of tasks and limited training data, such full fine-tuning approach leads to costly model storage and unstable training. To overcome these shortcomings, we introduce lightweight adapters to the pre-trained model and only update them at fine-tuning time. However, existing adapters fail to capture intrinsic temporal relations among video frames or textual words. Moreover, they neglect the preservation of critical task-related information that flows from the raw video-language input into the adapterâ€™s low-dimensional space. To address these issues, we first propose a novel REcurrent ADapter (READ) that employs recurrent computation to enable temporal modeling capability. Second, we propose Partial Video-Language Alignment (PVLA) objective via the use of partial optimal transport to maintain task-related information flowing into our READ modules. We validate our READ-PVLA framework through extensive experiments where READ-PVLA significantly outperforms all existing fine-tuning strategies on multiple low-resource temporal language grounding and video-language summarization benchmarks.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="nguyen2023demaformer" class="col-sm-8">
    
      <div class="title">DemaFormer: Damped Exponential Moving Average Transformer with Energy-Based Modeling for Temporal Language Grounding</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <span style="font-weight: 600;">Thong Nguyen</span>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiaobao Wu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xinshuai Dong,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Cong-Duy Nguyen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  See-Kiong Ng,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Luu Anh Tuan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of EMNLP (Findings)</em>
      
      
        <i>2023</i>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2312.02549" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Temporal Language Grounding seeks to localize video moments that semantically correspond to a natural language query. Recent advances employ the attention mechanism to learn the relations between video moments and the text query. However, naive attention might not be able to appropriately capture such relations, resulting in ineffective distributions where target video moments are difficult to separate from the remaining ones. To resolve the issue, we propose an energy-based model framework to explicitly learn moment-query distributions. Moreover, we propose DemaFormer, a novel Transformer-based architecture that utilizes exponential moving average with a learnable damping factor to effectively encode moment-query inputs. Comprehensive experiments on four public temporal language grounding datasets showcase the superiority of our methods over the state-of-the-art baselines.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACL</abbr>
    
  
  </div>

  <div id="nguyen2023gradient" class="col-sm-8">
    
      <div class="title">Gradient-Boosted Decision Tree for Listwise Context Model in Multimodal Review Helpfulness Prediction</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <span style="font-weight: 600;">Thong Nguyen</span>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiaobao Wu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xinshuai Dong,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Luu Anh Tuan,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Cong-Duy Nguyen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhen Hai,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Lidong Bing
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of ACL (Findings)</em>
      
      
        <i>2023</i>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2305.12678" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/nguyentthong/gbdt_listwise_mrhp" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Multimodal Review Helpfulness Prediction (MRHP) aims to rank product reviews based on predicted helpfulness scores and has been widely applied in e-commerce via presenting customers with useful reviews. Previous studies commonly employ fully-connected neural networks (FCNNs) as the final score predictor and pairwise loss as the training objective. However, FCNNs have been shown to perform inefficient splitting for review features, making the model difficult to clearly differentiate helpful from unhelpful reviews. Furthermore, pairwise objective, which works on review pairs, may not completely capture the MRHP goal to produce the ranking for the entire review list, and possibly induces low generalization during testing. To address these issues, we propose a listwise attention network that clearly captures the MRHP ranking context and a listwise optimization objective that enhances model generalization. We further propose gradient-boosted decision tree as the score predictor to efficaciously partition product reviewsâ€™ representations. Extensive experiments demonstrate that our method achieves state-of-the-art results and polished generalization performance on two large-scale MRHP benchmark datasets.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="nguyen2022adaptive" class="col-sm-8">
    
      <div class="title">Adaptive Contrastive Learning on Multimodal Transformer for Review Helpfulness Predictions</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <span style="font-weight: 600;">Thong Nguyen</span>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Xiaobao Wu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Luu Anh Tuan,
                
              
            
          
        
          
          
          
          
            
              
            
          
          
          
            
              
                
                  Cong-Duy Nguyen,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Zhen Hai,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Lidong Bing
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of EMNLP</em>
      
      
        <i>2022</i>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2211.03524" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/nguyentthong/adaptive_contrastive_mrhp" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Modern Review Helpfulness Prediction systems are dependent upon multiple modalities, typically texts and images. Unfortunately, those contemporary approaches pay scarce attention to polish representations of cross-modal relations and tend to suffer from inferior optimization. This might cause harm to modelâ€™s predictions in numerous cases. To overcome the aforementioned issues, we propose Multimodal Contrastive Learning for Multimodal Review Helpfulness Prediction (MRHP) problem, concentrating on mutual information between input modalities to explicitly elaborate cross-modal relations. In addition, we introduce Adaptive Weighting scheme for our contrastive learning approach in order to increase flexibility in optimization. Lastly, we propose Multimodal Interaction module to address the unalignment nature of multimodal data, thereby assisting the model in producing more reasonable multimodal representations. Experimental results show that our method outperforms prior baselines and achieves state-of-the-art results on two publicly available benchmark datasets for MRHP problem.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">AAAI</abbr>
    
  
  </div>

  <div id="nguyen2022improving" class="col-sm-8">
    
      <div class="title">Improving Neural Cross-Lingual Abstractive Summarization via Employing Optimal Transport Distance for Knowledge Distillation</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <span style="font-weight: 600;">Thong Nguyen</span>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Luu Anh Tuan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of AAAI</em>
      
      
        <i>2022</i>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2112.03473" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/nguyentthong/CrossSummOptimalTransport" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Current state-of-the-art cross-lingual summarization models employ multi-task learning paradigm,  which works on a shared vocabulary module and relies on the self-attention mechanism to attend among tokens in two languages. However, correlation learned by self-attention is often loose and implicit, inefficient in capturing crucial cross-lingual representations between languages. The matter worsens when performing on languages with separate morphological or structural features, making the cross-lingual alignment more challenging, resulting in the performance drop. To overcome this problem, we propose a novel Knowledge-Distillation-based framework for Cross-Lingual Summarization, seeking to explicitly construct cross-lingual correlation by distilling the knowledge of the monolingual summarization teacher into the cross-lingual summarization student. Since the representations of the teacher and the student lie on two different vector spaces, we further propose a Knowledge Distillation loss using Sinkhorn Divergence, an Optimal-Transport distance, to estimate the discrepancy between those teacher and student representations. Due to the intuitively geometric nature of Sinkhorn Divergence, the student model can productively learn to align its produced cross-lingual hidden states with monolingual hidden states, hence leading to a strong correlation between distant languages. Experiments on cross-lingual summarization datasets in pairs of distant languages demonstrate that our method outperforms state-of-the-art models under both high and low-resourced settings.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="nguyen2021contrastive" class="col-sm-8">
    
      <div class="title">Contrastive Learning for Neural Topic Model</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <span style="font-weight: 600;">Thong Nguyen</span>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Luu Anh Tuan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of NeurIPS</em>
      
      
        <i>2021</i>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2110.12764" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
      <a href="https://github.com/nguyentthong/CLNTM" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recent empirical studies show that adversarial topic models (ATM) can successfully capture semantic patterns of the document by differentiating a document with another dissimilar sample. However, utilizing that discriminative-generative architecture has two important drawbacks: (1) the architecture does not relate similar documents, which has the same document-word distribution of salient words; (2) it restricts the ability to integrate external information, such as sentiments of the document, which has been shown to benefit the training of neural topic model. To address those issues, we revisit the adversarial topic architecture in the view point of mathematical analysis, propose a novel approach to re-formulate discriminative goal as an optimization problem, and design a novel sampling method which facilitates the integration of external variables. The reformulation encourages the model to incorporate the relations among similar samples and enforces the constraint on the similarity among dissimilar ones; while the sampling method, which is based on the internal input and reconstructed output, helps inform the model of salient words contributing to the main topic. Experimental results show that our framework outperforms other state-of-the-art neural topic models in three common benchmark datasets that belong to various domains, vocabulary sizes, and document lengths in terms of topic coherence.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMNLP</abbr>
    
  
  </div>

  <div id="nguyen2021enriching" class="col-sm-8">
    
      <div class="title">Enriching and Controlling Global Semantics for Text Summarization</div>
      <div class="author">
        
          
          
          
          
            
              
                
                
          
          
          
            
              
                <span style="font-weight: 600;">Thong Nguyen</span>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Luu Anh Tuan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Truc Lu,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                 Tho Quan
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Proceedings of EMNLP</em>
      
      
        <i>2021</i>
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2109.10616" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Recently, Transformer-based models have been proven effective in the abstractive summarization task by creating fluent and informative summaries. Nevertheless, these models still suffer from the short-range dependency problem, causing them to produce summaries that miss the key points of document. In this paper, we attempt to address this issue by introducing a neural topic model empowered with normalizing flow to capture the global semantics of the document, which are then integrated into the summarization model. In addition, to avoid the overwhelming effect of global semantics on contextualized representation, we introduce a mechanism to control the amount of global semantics supplied to the text generation module. Our method outperforms state-of-the-art summarization models on five common text summarization datasets, namely CNN/DailyMail, XSum, Reddit TIFU, arXiv, and PubMed.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li></ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%76.%74%68%6F%6E%67%6E%74%36%36@%76%69%6E%61%69.%69%6F"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=C2zb0lkAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/nguyentthong" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>

<a href="https://twitter.com/thongnguyen5999" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>













      </div>
      <div class="contact-note"></div>
    </div>
    

  </article>
</div>

    <a href="javascript:toggleblock('notice')" style="text-align:center;font-size:70%;color:#808080">â–¶
      nguyentthong.github.io's clustrmaps ðŸŒŽ</a>
    <div id="notice" style="display: none; color: rgb(222, 222, 222); font-size: 0.5em;">
      <p>
        <a href="http://www.clustrmaps.com/map/Nguyentthong.github.io"
          title="Visit tracker for Nguyentthong.github.io"><img
            src="//www.clustrmaps.com/map_v2.png?d=I2YfwR8Kz4H9LoplfnONasdBtv6HkxEK5XkN7DoYPP8" /></a>
      </p>
    </div>
  </div>

  <!-- Footer -->

  
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2025 Thong T. Nguyen.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



</body>

<!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>


<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>





<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>